{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BirdsAI_ProdesDataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Bd-q1QJsntT6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1stijQKkKzM"
      },
      "source": [
        "\n",
        "# Time Series with Prodes Dataset\n",
        "[Prodes dataset](https://data.globalforestwatch.org/datasets/gfw::prodes-deforestation-in-amazonia/about)\n",
        "\n",
        "By Stephanie Mennear\n",
        "\n",
        "## Cleaning the data\n",
        "\n",
        "The Prodes data set contains many data points. From observing the dataset, most of the satellite images are taken from August - September, which are the months of the year that the Amazon is not mostly covered by clouds. It is during this time, that the satellite images can be taken for evaluation.\n",
        "\n",
        "Because the clouds obstruct the image of the satellite data, there are no consistent patterns for date ranges that the data is collected.\n",
        "\n",
        "To clean the data, the following functions drop all columns except the columns that hold values for States, date the images were taken, and the areakm.\n",
        "After looking at the satellite images, I came to the discovery that the areakm data points detailed the measurment of deforestation from the previous year. Therefore, after the data is cleaned, the cummulative sum is the value the time series forecast model will use.\n",
        "\n",
        "In order to prepare the dataset for the time series forecast model, the data must be prepared with a datetime index, as well as the data points to fit the date time index. Because of the inconsistant data points (due to cloud cover), the redisturbution method works as follows:\n",
        "\n",
        "1. The data points for each specific state of each specific year are redistributed to fit 365 days.\n",
        "2. The left over rows that did not fit into 365 days are summed, and divided amoung the 356 rows evenly. This means- each row value is added with the divided remaining value. This was to keep the integrity of the total amount of areakm that was deforested, and to prepare the dataset for yearly forecasting.\n",
        "3. Due to the method that it was distubuted, no daily, or monthly trends should be observed. Only yearly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs7VJW_tlEwm"
      },
      "source": [
        "## Prepare environment \n",
        "## Cleaning data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlkCzlUmkC2f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.filters.hp_filter import hpfilter\n",
        "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.stattools import acovf, acf, pacf, pacf_yw, pacf_ols\n",
        "from pandas.plotting import lag_plot\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tsa.ar_model import AR, ARResults\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tools.eval_measures import mse, rmse, meanabs\n",
        "!pip install pmdarima\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.arima_model import ARIMA, ARMA, ARMAResults, ARIMAResults\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxOu2_wFlaKG"
      },
      "source": [
        "- sum all the km for every year. \n",
        "- First, organize by year- collect dfs of each year. \n",
        "- second, get sum for each year. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaLulz50lVdI"
      },
      "source": [
        "df = pd.read_csv('PRODES_Deforestation_in_Amazonia.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-lvuct8lljj"
      },
      "source": [
        "def df_clean(df_):\n",
        "    df_.dropna(inplace=True)\n",
        "    df_.drop(columns=['FID', 'ORIGIN_ID', 'PATH_ROW', 'DEF_CLOUD', 'SCENE_ID', 'PUBLISH_YE', 'SOURCE', 'SATELLITE', 'SENSOR', 'MAIN_CLASS', 'CLASS_NAME', 'JULIAN_DAY', 'ID'], inplace=True)    \n",
        "    df_.sort_values(by='IMAGE_DATE', inplace=True)\n",
        "    df_['IMAGE_DATE'] = pd.to_datetime(df_['IMAGE_DATE'], format='%Y-%m-%d', errors='coerce')\n",
        "    df_.set_index(df_['IMAGE_DATE'], inplace=True)\n",
        "    df_.dropna(inplace=True)\n",
        "    org_yearly(df_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP5FtZV5lpxg"
      },
      "source": [
        "def org_yearly(df_):\n",
        "  df_name = {}\n",
        "  for year in df_.index.year.unique():\n",
        "    print('DF for {}'.format(year))\n",
        "    df_name[year] = df_['{}'.format(year)]\n",
        "    print(df_name[year])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUF8nBRslrqr"
      },
      "source": [
        "df_clean(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOXL9FVml7Fj"
      },
      "source": [
        "### Preparing for time models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuJYBEXel1nR"
      },
      "source": [
        "   - resample yearly\n",
        "   - Add cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxmSLnjkl9rg"
      },
      "source": [
        "df_y = df.resample('A').sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcn9gMdimDLO"
      },
      "source": [
        "df_y['cumsum'] = df_y.AREA_KM.cumsum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSfkJ2K5mDwf"
      },
      "source": [
        "df_y['cumsum'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d12IlyCmPwP"
      },
      "source": [
        "### Time Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPiIm1LGmN6p"
      },
      "source": [
        "df_y.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Cq0dGMmN9W"
      },
      "source": [
        "train_yearly = df_y.iloc[:9]\n",
        "test_yearly = df_y.iloc[8:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AgmwrRqmOCL"
      },
      "source": [
        "fitted_model = ExponentialSmoothing(train_yearly['cumsum'], trend='mul', seasonal_periods=1).fit()\n",
        "test_predictions = fitted_model.forecast(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "godx4OrymOEa"
      },
      "source": [
        "train_yearly['cumsum'].plot(figsize=(12,5), legend=True, label='Train')\n",
        "test_yearly['cumsum'].plot(legend=True, label='Test')\n",
        "test_predictions.plot(legend=True, label='Prediction')\n",
        "plt.title('Predictions from 2017 - 2026 \\nAreakm of total deforestation \\n All 9 State of Legal Amazonia', fontsize=20)\n",
        "plt.xlabel('Years', fontsize=12)\n",
        "plt.ylabel('Areakm total deforestation', fontsize=12)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTi5C6MmOG5"
      },
      "source": [
        "test_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSCmjmL_mdgz"
      },
      "source": [
        "df_y['cumsum']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZOE36r2mh7n"
      },
      "source": [
        "- put text preditions in a dataframe to download and send to flask server\n",
        "<br>\n",
        "- also add non predictions in a csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQhoPFVYmdi1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19VtF-iSmdlm"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMsUCY9Bmpih"
      },
      "source": [
        "recorded_areakm = df_y['cumsum'].to_frame()\n",
        "recorded_areakm.reset_index(level=0, inplace=True)\n",
        "recorded_areakm.columns=['Recorded Years', 'Recorded Areakm Deforested']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMTcyWbCmplR"
      },
      "source": [
        "RecordedAreakm = recorded_areakm.to_csv()\n",
        "with open('RecordedAreakm.csv', 'a') as f:\n",
        "  f.write(RecordedAreakm)\n",
        "files.download('RecordedAreakm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTpW5fe_mpno"
      },
      "source": [
        "test_predictions_df=test_predictions.to_frame()\n",
        "test_predictions_df.reset_index(level=0, inplace=True)\n",
        "test_predictions_df.columns=['Prediction Year', 'Predicted Areakm Deforested']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0nbMFUCmppu"
      },
      "source": [
        "Brazil_ESM = test_predictions_df.to_csv()\n",
        "with open('BrazilESM.csv', 'a') as f:\n",
        "  f.write(Brazil_ESM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwL2no0xpGf4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USJQIueympsJ"
      },
      "source": [
        "files.download('BrazilESM.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd-q1QJsntT6"
      },
      "source": [
        "\n",
        "#### Failed model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHVx4orVnQL-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RTtOG-rnRln"
      },
      "source": [
        "result_y = seasonal_decompose(df_y['cumsum'], model='multiplicative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR9iz_C-oLhV"
      },
      "source": [
        "result_y.plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZMs4x8SoPXQ"
      },
      "source": [
        "auto_arima(df_y['cumsum']).summary() #m is for the seasonal periods\n",
        "#SARIMAX(0, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcJuzn0FoPZs"
      },
      "source": [
        "train_sy = df_y.iloc[:10]\n",
        "test_sy = df_y.iloc[10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZTfW1K4oPb3"
      },
      "source": [
        "model_sy = SARIMAX(train_sy['cumsum'], order=(0,1,0))\n",
        "result_sy = model_sy.fit()\n",
        "start_sy = len(train_sy)\n",
        "end_sy = len(train_sy) + len(test_sy) -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh6YbaTAoPeP"
      },
      "source": [
        "prediction_sy = result_sy.predict(start_sy, end_sy, type='levels').rename('SARIMAX Yearly Predictions')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKafpIXBoPqE"
      },
      "source": [
        "test_sy['cumsum'].plot(legend= True, figsize=(12, 8))\n",
        "prediction_sy.plot(legend=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE5FGdhWopju"
      },
      "source": [
        "#### Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw1GYcrgqo8-"
      },
      "source": [
        "#### Rate of deforestation over time from 2008 - 2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AD2pVUyox8U"
      },
      "source": [
        "df.median()\n",
        "df.describe()\n",
        "df['STATE'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUSsXdglo9tP"
      },
      "source": [
        "state_group = df.groupby(['STATE'])\n",
        "state_group['AREA_KM'].agg(['median', 'mean', 'sum'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "463AMbp_pA6S"
      },
      "source": [
        "state_group.get_group('AM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOAWB6RlpCqc"
      },
      "source": [
        "group_year = df.groupby(['IMAGE_DATE'])\n",
        "group_year.get_group(2008)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZydpLRSpI4j"
      },
      "source": [
        "group_year['STATE'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xYMhmF6pLR8"
      },
      "source": [
        "group_year['AREA_KM'].sum() #.plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3uHjsBZpNJZ"
      },
      "source": [
        "grp = df.groupby(by=['YEAR', 'STATE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGtMtN_wpNn_"
      },
      "source": [
        "grp['AREA_KM'].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIuwvbxmpNqb"
      },
      "source": [
        "grp['AREA_KM'].sum().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc-GMlaeqRPj"
      },
      "source": [
        "\n",
        "- The above visualizes the years 2008-2019. It is not clear by the marks, so for readability, each group of peaks represents a year. \n",
        "- From this visualization, 2008 is extremley high, while trends dip from 2009 - 2017.\n",
        "- Discovering from research, Brazil saw a decline in deforestation rates from 2008 to around 2018.\n",
        "  According to an article published November 2020 by [BBC](https://www.bbc.com/news/world-latin-america-55130304), in collaboration with Prodes data:\n",
        "  > \"Amazon deforestation highest since 2008\"\n",
        "\n",
        "Why? [BBC](https://www.bbc.com/news/world-latin-america-55130304) states:\n",
        "\n",
        "> \"Scientists say it has suffered losses at an accelerated rate since Jair Bolsonaro took office in January 2019.\n",
        "> The Brazilian president has encouraged agriculture and mining activities in the world's largest rainforest.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi5vlZBWqufb"
      },
      "source": [
        "#### Rate of deforestation by state in total from 2008-2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZA1E-qipNsb"
      },
      "source": [
        "state_area = df.groupby(by='STATE')['AREA_KM'].sum().sort_values(ascending=False).reset_index()\n",
        "state_area = state_area.sort_values(by='AREA_KM', ascending=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3rqH33drMQk"
      },
      "source": [
        "Next step is to figure out size of each state and find the percentage that it has been deforested, then to look again at the rates of reforestation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdnC3u0PrI1P"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "ax = sns.barplot(x=state_area['STATE'], y=state_area['AREA_KM'], palette='Greens', alpha=0.85)\n",
        "plt.title(\"Deforestation by State\", fontsize = 25)\n",
        "plt.xlabel(\"State\", fontsize = 20)\n",
        "plt.ylabel(\"Sum of km2 deforestation\", fontsize = 20)\n",
        "plt.xticks(fontsize = 15)\n",
        "plt.yticks(fontsize = 15)\n",
        "plt.legend(fontsize = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74DBteu7sKiU"
      },
      "source": [
        "**A similar process can be seen under the Failed Methods section: \"Visualization of total cumsum deforested per state\"**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJjdrhLfsUi_"
      },
      "source": [
        "# Failed Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1zmyIBYsFKf"
      },
      "source": [
        "## Ceaning the data\n",
        "**A way of cleaning data, before fully understanding the capabilites of pandas dataframe methods offered**... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vjTTqFKrSd7"
      },
      "source": [
        "df = pd.read_csv('PRODES_Deforestation_in_Amazonia.csv')\n",
        "def df_clean(df_):\n",
        "    df_.dropna(inplace=True)\n",
        "    df_.drop(columns=['FID', 'ORIGIN_ID', 'PATH_ROW', 'DEF_CLOUD', 'SCENE_ID', 'PUBLISH_YE', 'SOURCE', 'SATELLITE', 'SENSOR', 'MAIN_CLASS', 'CLASS_NAME', 'JULIAN_DAY', 'ID'], inplace=True)    \n",
        "    df_.sort_values(by='IMAGE_DATE', inplace=True)\n",
        "    org_state(df_) #next function\n",
        "#organize the df to clean for processing.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3dPp2_8rSfy"
      },
      "source": [
        "#organize the df into states\n",
        "#the individual df of indv. states will pass through to the next function, bringing the state name with it\n",
        "def org_state(df_):\n",
        "    df_.dropna(inplace=True) #put it here for further caution\n",
        "    df_name = {}\n",
        "    for state in df_.STATE.unique():       \n",
        "        df_name[state] = df_.loc[df_['STATE'] == state].copy()\n",
        "        print('********************', '\\n', 'Information for state of {} :'.format(state), '\\n', '********************', '\\n')\n",
        "        clean_date(df_name[state], state) #next function\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgiM1OjxrSiD"
      },
      "source": [
        "#change the Image_date to datetime\n",
        "#individual df of the individual year will go through to the next function\n",
        "#here a date range is created as well, this is carried through to the next function\n",
        "def clean_date(df_, state):\n",
        "    df_.dropna(inplace=True)\n",
        "    df_year = {}\n",
        "    state = state\n",
        "    #turn image_date to a time series to get information about it and seperate it\n",
        "    df_['IMAGE_DATE'] = pd.to_datetime(df_['IMAGE_DATE'], format='%Y-%m-%d', errors='coerce').copy()\n",
        "    #time to organize the datetimes into seperate dfs. for each year\n",
        "    df_.set_index(df_['IMAGE_DATE'], inplace=True)\n",
        "    df_.dropna(inplace=True)   \n",
        "    for year in df_.index.year.unique():\n",
        "      print('*************** CLEANING FOR {}'.format(year), '\\n')\n",
        "      year = int(year)\n",
        "      df_year[year] = df_.loc[df_.YEAR == year].copy()\n",
        "      global date_rng\n",
        "      date_rng = pd.date_range(start='{}-01-01'.format(year), end='{}-12-31'.format(year), freq='D') #int year to sove the float problem\n",
        "      #print('date range length: ', len(date_rng))\n",
        "      apply_new_dates(df_year[year], date_rng, year, state)\n",
        "      \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsK1iQl0sqFN"
      },
      "source": [
        "#the individual state/year df will be processed to disperse across the date_range.\n",
        "def apply_new_dates(df_, date_ranges, year, state):\n",
        "  df_.dropna(inplace=True)\n",
        "  unique_df_id = '{}'.format(state) + '{}'.format(year)\n",
        "  global cleaned_date_list \n",
        "  cleaned_date_list = []\n",
        "  global popped_numbers \n",
        "  popped_numbers = []\n",
        "  df_Htime = {}\n",
        "  subtract = {}\n",
        "  Y = {}\n",
        "  series_area = df_['AREA_KM']\n",
        "  if(len(series_area) > len(date_ranges)):\n",
        "    subtract_by = len(series_area) - len(date_ranges)\n",
        "    #print(subtract_by)\n",
        "    poped_rows = series_area[len(date_ranges):] #all the rows that cant fit to date range\n",
        "    rows_hourly = series_area[:(len(date_ranges))] #all the rows that fit in date range\n",
        "    #average = sum(poped_rows)/len(poped_rows)\n",
        "    average = sum(poped_rows) #add all the values together\n",
        "    dispurse_number = average / len(rows_hourly) #divide by rows in the timerange #len(poped_rows)\n",
        "    lst_date_rng = date_rng.tolist()\n",
        "    hourly_list = rows_hourly.tolist()\n",
        "    df_Htime[unique_df_id] = pd.DataFrame({'date':lst_date_rng, 'areakm':hourly_list, 'state':state})\n",
        "    disperse_areakms(df_Htime[unique_df_id], dispurse_number) #next function to disperse the left over numbers\n",
        "    create_big_csv(df_Htime[unique_df_id]) #next function to add it to a file\n",
        "  else:\n",
        "    area_sum = df_['AREA_KM'].sum()\n",
        "    dispurse_number = area_sum / len(date_ranges)\n",
        "    #print('Date range length: ', len(date_ranges))\n",
        "    #print('DF length: ', len(series_area))\n",
        "    #series_dates = date_ranges.to_series() #in order to merge into a df\n",
        "    lst_date_rng = date_ranges.to_list() #change it to list instead so index does not come with it\n",
        "    df_Htime[unique_df_id] = pd.DataFrame({'date':lst_date_rng, 'areakm':0, 'state':state})\n",
        "    df_Htime[unique_df_id].fillna(0, inplace=True)\n",
        "    #df_Htime[unique_df_id]['areakm'] = df_Htime[unique_df_id]['areakm'] + disperser\n",
        "    #send it to the function call instead\n",
        "    disperse_areakms(df_Htime[unique_df_id], dispurse_number) #next function to disperse the left over numbers \n",
        "    create_big_csv(df_Htime[unique_df_id]) #next function to add it to a file\n",
        " \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S0S-wKmstTN"
      },
      "source": [
        "def disperse_areakms(df_, disperser):\n",
        "  df_['areakm'] = df_['areakm'] + disperser\n",
        "  print(df_.head())\n",
        "  return(df_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T25cE0YQstYa"
      },
      "source": [
        "df_clean(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1D1kkGbstc1"
      },
      "source": [
        "#Load the csv into a dataframe\n",
        "#Take headers out\n",
        "#shift first column out of header column\n",
        "#rename the headers\n",
        "#set index\n",
        "df2 = pd.read_csv('df_dattime_byStateYears1.csv', header=None)\n",
        "df2.rename(columns={0:'date', 1:'areakm', 2:'state'}, inplace=True)\n",
        "df2['date'] = pd.to_datetime(df2['date'], format='%Y-%m-%d', errors='coerce')\n",
        "df2.set_index(df2['date'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU81eJYhsthN"
      },
      "source": [
        "df2.isnull().values.sum()\n",
        "df2.isna().values.sum()\n",
        "df2.state.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26X6jhn_stk-"
      },
      "source": [
        "#all states are complete and in order of time. \n",
        "df_pa = df2.loc[df2['state'] == 'PA'] \n",
        "df_am = df2.loc[df2['state'] == 'AM'] \n",
        "df_rr = df2.loc[df2['state'] == 'RR'] \n",
        "df_ro = df2.loc[df2['state'] == 'RO'] \n",
        "df_mt = df2.loc[df2['state'] == 'MT'] \n",
        "df_ac = df2.loc[df2['state'] == 'AC'] \n",
        "df_ma = df2.loc[df2['state'] == 'MA'] \n",
        "df_to = df2.loc[df2['state'] == 'TO'] \n",
        "df_ap = df2.loc[df2['state'] == 'AP'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Z8nj0HtGyp"
      },
      "source": [
        "## Visualization of total cumsum deforested per state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZII48o3stoI"
      },
      "source": [
        "df_pa['areasum'] = df_pa.areakm.cumsum()\n",
        "df_am['areasum'] = df_am.areakm.cumsum()\n",
        "df_rr['areasum'] = df_rr.areakm.cumsum() \n",
        "df_ro['areasum'] = df_ro.areakm.cumsum()\n",
        "df_mt['areasum'] = df_mt.areakm.cumsum()\n",
        "df_ac['areasum'] = df_ac.areakm.cumsum()\n",
        "df_ma['areasum'] = df_ma.areakm.cumsum() \n",
        "df_to['areasum'] = df_to.areakm.cumsum() \n",
        "df_ap['areasum'] = df_ap.areakm.cumsum() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S-CJ1WutOY0"
      },
      "source": [
        "#make a bif df with all states once again\n",
        "frames = [df_pa, df_am, df_rr, df_ro, df_mt, df_ac, df_ma, df_to, df_ap]\n",
        "df_cs = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMkt51hCtOeO"
      },
      "source": [
        "areasum_states = df_cs.groupby(by='state')['areasum'].sum().sort_values(ascending=False).reset_index()\n",
        "areasum_states = areasum_states.sort_values(by='areasum', ascending=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHGWdslttOiI"
      },
      "source": [
        "plt.figure(figsize = (16, 9))\n",
        "\n",
        "# plot\n",
        "ax = sns.barplot(x = areasum_states['state'], y = areasum_states['areasum'], palette = \"Reds\", alpha = 0.85)\n",
        "\n",
        "\n",
        "plt.title(\"States\", fontsize = 25)\n",
        "plt.xlabel(\"State\", fontsize = 20)\n",
        "plt.ylabel(\"areasum\", fontsize = 20)\n",
        "plt.xticks(fontsize = 15)\n",
        "plt.yticks(fontsize = 15)\n",
        "plt.legend(fontsize = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN3MwCjktOoq"
      },
      "source": [
        "sns.lineplot(data=df_cs, x='date', y='areasum', hue='state')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sruhZtnTtOrb"
      },
      "source": [
        "fig, axs = plt.subplots(3, 3, sharex='col', sharey='row')\n",
        "axs[0, 0].plot(df_pa['date'], df_pa['areasum'])\n",
        "axs[0, 0].set_title('PA')\n",
        "axs[0, 1].plot(df_am['date'], df_am['areasum'])\n",
        "axs[0,1].set_title('AM')\n",
        "axs[0, 2].plot(df_rr['date'], df_rr['areasum'])\n",
        "axs[0,2].set_title('RR')\n",
        "axs[1, 0].plot(df_ro['date'], df_ro['areasum'])\n",
        "axs[1,0].set_title('RO')\n",
        "axs[1, 1].plot(df_mt['date'], df_mt['areasum'])\n",
        "axs[1, 1].set_title('MT')\n",
        "axs[1, 2].plot(df_ac['date'], df_ac['areasum'])\n",
        "axs[1, 2].set_title('AC')\n",
        "axs[2, 0].plot(df_ma['date'], df_ma['areasum'])\n",
        "axs[2,0].set_title('MA')\n",
        "axs[2, 1].plot(df_to['date'], df_to['areasum'])\n",
        "axs[2,1].set_title('TO')\n",
        "axs[2, 2].plot(df_ap['date'], df_ap['areasum'])\n",
        "axs[2,2].set_title('AP')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_CwpM0jtxx8"
      },
      "source": [
        "## Failed Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up1e8R1Yt6dF"
      },
      "source": [
        "Testing models on the state of Amazonia over the years of 2008-2019. df_am\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8yWgHTcuOWY"
      },
      "source": [
        "### Resampling, monthly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ44h75Dt80Z"
      },
      "source": [
        "df_am_month = df_am.resample(rule='MS').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZVXi0G9t84s"
      },
      "source": [
        "train_monthly = df_am_month.iloc[:121]\n",
        "test_monthly = df_am_month.iloc[120:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9zjK_mt88j"
      },
      "source": [
        "fitted_model = ExponentialSmoothing(train_monthly['areasum'], trend='add', seasonal_periods=12).fit()\n",
        "test_predictions = fitted_model.forecast(24) #2 years into future"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZBHIWFpt9As"
      },
      "source": [
        "#plot them all\n",
        "train_monthly['areasum'].plot(figsize=(12,5), legend=True, label='Train')\n",
        "test_monthly['areasum'].plot(legend=True, label='Test')\n",
        "test_predictions.plot(legend=True, label='Prediction')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaHkWTk8uhcj"
      },
      "source": [
        "### Yearly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1QA9IoNt9E3"
      },
      "source": [
        "df_am_year = df_am.resample(rule='A').mean()\n",
        "df_am_year.plot()\n",
        "#areakm is white noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxZRzrlyt9Iy"
      },
      "source": [
        "train_yearly = df_am_year.iloc[:9]\n",
        "test_yearly = df_am_year.iloc[8:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7LJj8S5urPr"
      },
      "source": [
        "#fitted_model = ExponentialSmoothing(train_monthly['areasum'], trend='add', seasonal_periods=12).fit()\n",
        "fitted_model = ExponentialSmoothing(train_yearly['areasum'], trend='add', seasonal_periods=1).fit()\n",
        "test_predictions = fitted_model.forecast(10) #2 years into future"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJPghDlMurTq"
      },
      "source": [
        "train_yearly['areasum'].plot(figsize=(12,5), legend=True, label='Train')\n",
        "test_yearly['areasum'].plot(legend=True, label='Test')\n",
        "test_predictions.plot(legend=True, label='Prediction')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7b1UCBAuws1"
      },
      "source": [
        "### Sarimax with Monthly data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIky5agWurXp"
      },
      "source": [
        "#look at the seasonal decompose:\n",
        "result_m = seasonal_decompose(df_am_month['areasum'], model='add')\n",
        "result_m.plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA7vN2gnu3Mu"
      },
      "source": [
        "auto_arima(df_am_month['areasum'], seasonal=True, m=12).summary() #monthly data, 12 rows per year"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5w54pYju3Ts"
      },
      "source": [
        "#forecast into future- length of dataset is 144 rows\n",
        "#train_sm = train Sarimax Monthly\n",
        "train_sm = df_am_month.iloc[:132] #minus the last year\n",
        "test_sm = df_am_month.iloc[132:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16DSzrTku3Yz"
      },
      "source": [
        "model = SARIMAX(train_sm['areasum'], order=(0,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCo4GnSau9yE"
      },
      "source": [
        "result_sm = model.fit()\n",
        "result_sm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9cMNOeiu91W"
      },
      "source": [
        "start_sm = len(train_sm)\n",
        "end_sm = len(train_sm) + len(test_sm) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2RyySSuu934"
      },
      "source": [
        "#now create the predictions\n",
        "prediction_sm = result_sm.predict(start_sm, end_sm, type='levels').rename('Sarima Monthly predications')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8F_5n_ru96a"
      },
      "source": [
        "test_sm['areasum'].plot(legend=True, figsize=(12, 8))\n",
        "prediction_sm.plot(legend=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzWY5rklu98y"
      },
      "source": [
        "fcast = result_sm.predict(len(df_am_month), len(df_am_month) + 11, type='levels').rename('Sarima Forecast')\n",
        "test_sm['areasum'].plot(legend=True, figsize=(12, 8))\n",
        "prediction_sm.plot(legend=True)\n",
        "fcast.plot(legend=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4H7421IvM78"
      },
      "source": [
        "### Sarimax with Yearly data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8-dcWfjvRe7"
      },
      "source": [
        "result_y = seasonal_decompose(df_am_year['areasum'], model='add')\n",
        "result_y.plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4NzeSXavRiP"
      },
      "source": [
        "auto_arima(df_am_year['areasum'], m=1).summary()\n",
        "#SARIMAX(2, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhRW9-TYvRn0"
      },
      "source": [
        "train_sy = df_am_year.iloc[:10] #minus the last year\n",
        "test_sy = df_am_year.iloc[10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYqgFfMCvRrc"
      },
      "source": [
        "model_y = SARIMAX(train_sy['areasum'], order=(2,1,0))\n",
        "resuly_y = model_y.fit()\n",
        "start_sy = len(train_sy)\n",
        "end_sy = len(train_sy) + len(test_sy) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw2fsDULvR0W"
      },
      "source": [
        "#now create the predictions\n",
        "prediction_sy = resuly_y.predict(start_sy, end_sy, type='levels').rename('Sarima Yearly predications')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kX3KSvSvaNC"
      },
      "source": [
        "test_sy['areasum'].plot(legend=True, figsize=(12, 8))\n",
        "prediction_sy.plot(legend=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej3xK4dpvaQo"
      },
      "source": [
        "#future\n",
        "fcast_y = resuly_y.predict(len(df_am_year), len(df_am_year) + 3, type='levels').rename('Sarima Forecast')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PCK6li8vaTb"
      },
      "source": [
        "test_sy['areasum'].plot(legend=True, figsize=(12, 8))\n",
        "prediction_sy.plot(legend=True)\n",
        "fcast_y.plot(legend=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrtCIBxJvvy4"
      },
      "source": [
        "### More failed models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6GmIJF4v3Oi"
      },
      "source": [
        "Notes: \n",
        "- choosing arma/arima orders. chooseing the best p,q,a\n",
        "- finding out the orders of the ar and ma components.\n",
        "- finding out if the i component is needed\n",
        "- if the aurocorrelation plot shows positive autocorrelation at first lag (lag-1), suggested to use AR terms in relation to lag\n",
        "- if the autocorrelation plot shows negative autocorrelation, suggest using MA terms\n",
        "- p: number of lag observations included in ar component of the model\n",
        "- d: number of times the raw observations are differenced\n",
        "- q: size of moving average window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td1EcB0SzUlz"
      },
      "source": [
        "##Arima\n",
        "non seasonal arima (p, d, q) </br>\n",
        "p = corresponds to the AR poriont of model</br>\n",
        "d = Indegrated componend. Differencing- diff of observations. In order to make the time series stationary (statsmodels as the diff function) </br>\n",
        "q =  corresponds to MA component. Plotting out moving average, and using the residual error </br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlIJRGupwpbb"
      },
      "source": [
        "def adf_test(series, title=''):\n",
        "  print(f'Augmented Dickey-Fuller Test: {title}')\n",
        "  result=adfuller(series.dropna(), autolag='AIC')\n",
        "  labels=['ADF test stats', 'p-value', 'number of lags used', 'number of observations']\n",
        "  out = pd.Series(result[0:4], index=labels)\n",
        "  for key, val in result[4].items():\n",
        "    out[f'critical value ({key})'] = val\n",
        "  \n",
        "  print(out.to_string())\n",
        "\n",
        "  if result[1] <= 0.05:\n",
        "    print('Strong evidence against null hypthesis.', '\\n', 'Reject null hypothesis.', '\\n', 'Data has no unit root and is stationary')\n",
        "  else:\n",
        "    print('Weake evidence against null hypthesis.', '\\n', 'Fail to reject null hypothesis.', '\\n', 'Data has a unit root and is non-stationary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRev4kgrzZVY"
      },
      "source": [
        "model = AR(train_data['areasum'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3hogTaHzn73"
      },
      "source": [
        "model = AR(train_data['areasum'])\n",
        "AR1fit.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TVUuJN5zoEv"
      },
      "source": [
        "start = len(train_data)\n",
        "end = len(train_data) + len(test_data) -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1rs1r90zoKO"
      },
      "source": [
        "AR1fit.predict(start=start, end=end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZHHcZ6yzvf-"
      },
      "source": [
        "#compare predicted values to real known test values\n",
        "prediction1 = AR1fit.predict(start=start, end=end)\n",
        "#name it to keep track\n",
        "prediction1 = prediction1.rename('AR(1) Predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDUxDsbWzvsX"
      },
      "source": [
        "test_data['areasum'].plot(figsize=(12,8), legend=True, label='Origional')\n",
        "prediction1.plot(legend=True)\n",
        "#under predicting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU1_aV9hzv0P"
      },
      "source": [
        "#try to improve this by expanding order\n",
        "#name it to keep track\n",
        "AR2fit = model.fit(maxlag=2)\n",
        "AR2fit.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVRScRRzzv7o"
      },
      "source": [
        "prediction2 = AR2fit.predict(start=start, end=end)\n",
        "prediction2 = prediction2.rename('AR2 predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLMhPT1Dz28o"
      },
      "source": [
        "#now plot the three\n",
        "test_data['areasum'].plot(figsize=(12,8), legend=True, label='Origional')\n",
        "prediction1.plot(legend=True)\n",
        "prediction2.plot(legend=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEOyFpK4z3CM"
      },
      "source": [
        "#finidng the correct order value, let statsmodels decide, do not specify maxlag\n",
        "#look into the ic parameter\n",
        "ARfit = model.fit(ic='t-stat')\n",
        "ARfit.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baFq_GvDz3G8"
      },
      "source": [
        "prediction28 = ARfit.predict(start=start, end=end)\n",
        "prediction28 = prediction28.rename('AR28 Predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMS_b0xqz3L7"
      },
      "source": [
        "#evaluate it\n",
        "labels = ['AR1', 'AR2', 'AR28']\n",
        "preds = [prediction1, prediction2, prediction28]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpaqazKpz3Pw"
      },
      "source": [
        "for i in range(3):\n",
        "  #np.sqrt() #use if you want\n",
        "  error = mean_squared_error(test_data['areasum'], preds[i])\n",
        "  print(f'{labels[i]} error: {error}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIe2aKQq0Fym"
      },
      "source": [
        "#plot all 4\n",
        "test_data['areasum'].plot(figsize=(12,8), legend=True, label='Origional')\n",
        "prediction1.plot(legend=True)\n",
        "prediction2.plot(legend=True)\n",
        "prediction28.plot(legend=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC2u0zqRwd3b"
      },
      "source": [
        "#### Arma with df_am_year1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeSNMpJWvaWE"
      },
      "source": [
        "#we know seasonal is false\n",
        "#trace will show you the first few arima models that it is trying to fix\n",
        "stepwise_fit = auto_arima(df_am['areasum'], start_P=0, start_q=0, max_p=6, max_q=3, seasonal=False, trace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oOV0KYqwLdY"
      },
      "source": [
        "stepwise_fit.summary()\n",
        "#why does it suggest a SARIMA model? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvWBmE1lwLgW"
      },
      "source": [
        "#look at first year\n",
        "df_am_year1 = df_am[:366]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VbZEhfcwLlv"
      },
      "source": [
        "df_am_year1.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8be1joIwLoM"
      },
      "source": [
        "df_am_year1['areakm'].plot(figsize=(12,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B10mdzeIwhl7"
      },
      "source": [
        "adf_test(df_am_year1['areakm'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRIv3_etwhoZ"
      },
      "source": [
        "auto_arima(df_am_year1['areakm'], seasonal=False).summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dm0GwZ1whq7"
      },
      "source": [
        "auto_arima(df_am['areakm'], seasonal=False).summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTlX43tgw1J7"
      },
      "source": [
        " Failed, moving on to **ARIMA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT6DUT0Qw8jY"
      },
      "source": [
        "#### ARIMA with df_am"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2vJK7kcw3lc"
      },
      "source": [
        "using df_am"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnuvL9Jrwhue"
      },
      "source": [
        "test_am = df_am[:3650]\n",
        "train_am = df_am[3650:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUjsl6GVwhxS"
      },
      "source": [
        "model = ARIMA(train_am['areakm'], order=(2,1,3))\n",
        "results = model.fit()\n",
        "results = model.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2vUYDygwhzi"
      },
      "source": [
        "#set start and end location (dates)\n",
        "start_am = len(train_am)\n",
        "end_am = len(train_am) + len(test_am) -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQzXWyNNwh2d"
      },
      "source": [
        "predictions = results.predict(start_am,end_am).rename('ARIMA(2,1,3)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Dfx39FxZIp"
      },
      "source": [
        "test_am['areakm']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DfAk8dyxZLK"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6x16_vDxZN2"
      },
      "source": [
        "test_am['areakm'].plot(figsize=(12,8), legend=True)\n",
        "predictions.plot(legend=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytt47OdwxhxO"
      },
      "source": [
        "### Resample yearly, SARIMAX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHAYTlUdxZQS"
      },
      "source": [
        "df_am_yearly = df_am.resample(rule='A').sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhQddKuexZTO"
      },
      "source": [
        "df_am_yearly.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX93xFIRxZVg"
      },
      "source": [
        "results3 = auto_arima(df_am_yearly, start_P=0, start_q=0, max_p=6, max_q=3, seasonal=False, trace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNsH7VczxomX"
      },
      "source": [
        "results3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "patKWZdwxvef"
      },
      "source": [
        "- the time series is white noise?\n",
        "- mean = 0\n",
        "- std is contant with time\n",
        "- correlatoin lag between lags is 0\n",
        "- is it stationary?\n",
        "- mean is constant\n",
        "- std is constant\n",
        "- no seasonality\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx0GOIs3xouJ"
      },
      "source": [
        "smodel = SARIMAX(train_am['areakm'], order=(0,1,3), enforce_invertibility=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ot7ul-Axoyf"
      },
      "source": [
        "results = smodel.fit()\n",
        "results.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clRSdOL3xo1x"
      },
      "source": [
        "start = len(train_am)\n",
        "end = len(train_am) +len(test_am) -1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b1nTdasxo4c"
      },
      "source": [
        "predictions = results.predict(start,end).rename('Sarima')\n",
        "predictions = results.predict(start,end).rename('Sarima')\n",
        "test_am['areakm'].plot(figsize=(15,8))\n",
        "predictions.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wbDks8fybNC"
      },
      "source": [
        "# Evaluation Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc7U0VPlyhJa"
      },
      "source": [
        "###ACF and PACF\n",
        "-autocorrelation and partial autocorrelation tests </br>\n",
        "-correlation: closer to 0, weak relationship. +1 / -1 negativly or positively correlated\n",
        "\n",
        "- autocorrelation, compares a series to itself, lagged"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZhNOmKgyebP"
      },
      "source": [
        "#check if frequency is set\n",
        "#if none, then set it\n",
        "df_am.index.freq= 'D'\n",
        "print(df_am.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5mERPhBykek"
      },
      "source": [
        "#auto corelation\n",
        "acf(df_am['areasum'])\n",
        "#positivly correlated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOr-BnROykl1"
      },
      "source": [
        "lag_plot(df_am['areasum'])\n",
        "#is this correct? this is purely strong autocorrelation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_wPqRdTykpW"
      },
      "source": [
        "plot_acf(df_am['areasum'], lags=40);\n",
        "#confidence interval - correlation values outside of it, highly likely to be a correlation. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1LjQ41YyksL"
      },
      "source": [
        "#look at the same plot on the areakm column\n",
        "plot_acf(df_am['areakm'], lags=40);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8UGMcpwykxa"
      },
      "source": [
        "#what is this? \n",
        "df_am['areakm'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytsfozljyk0u"
      },
      "source": [
        "#PACF\n",
        "#stationary\n",
        "#if a sharp cutoff, indicator to add pr terms\n",
        "plot_pacf(df_am['areakm'], lags=40);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBEJtxCdyk3f"
      },
      "source": [
        "#non stationary\n",
        "plot_pacf(df_am['areasum'], lags=40);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuYDxnIdyk6G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgPNgPmFyk8d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0NIceWtylA1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOVj8jkfylD6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}